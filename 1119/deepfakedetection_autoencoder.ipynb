{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/DZpm1Aukd56NcfWqzDKd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongjinkong/opensw/blob/main/1119/deepfakedetection_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mystlee/dfdc_deepfake_challenge.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rX1Hdsg1_gy",
        "outputId": "15fe7697-6b56-4653-d13f-98630034bfaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dfdc_deepfake_challenge'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 134 (delta 23), reused 4 (delta 4), pack-reused 65 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 69.20 MiB | 19.94 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow\n",
        "!pip install -U keras\n",
        "!pip install efficientnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QAAU7xk7mv5c",
        "outputId": "ab7e565a-0890-4671-c924-955fdb024306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              },
              "id": "87a9502780784ac6bba4fee4d292fc22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -eras (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting efficientnet\n",
            "  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet) (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.12.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (0.4)\n",
            "Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKCllYhbjYWc",
        "outputId": "00b8983b-7c38-4710-e439-47a1a7eb00df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZk0R6od3z_4",
        "outputId": "67f336c3-ec61-400c-bcd2-e48de0f2d9e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.6.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# 이미지 로딩 및 전처리 함수\n",
        "def load_images(image_paths, target_size=(128, 128)):\n",
        "    images = []\n",
        "    for img_path in image_paths:\n",
        "        img = load_img(img_path, target_size=target_size)\n",
        "        img = img_to_array(img) / 255.0  # 정규화\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Google Drive에서 데이터 로드 및 준비\n",
        "deepfake_folder = '/content/drive/MyDrive/deepfake/'  # 실제 경로로 변경\n",
        "image_paths = [os.path.join(deepfake_folder, f) for f in os.listdir(deepfake_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# 이미지 데이터 로드\n",
        "x_images = load_images(image_paths)\n",
        "\n",
        "# 라벨 생성 (FAKE: 0, REAL: 1)\n",
        "y_labels = []\n",
        "for img_path in image_paths:\n",
        "    if 'fake' in img_path.lower():\n",
        "        y_labels.append(0)  # FAKE\n",
        "    else:\n",
        "        y_labels.append(1)  # REAL\n",
        "y_labels = np.array(y_labels)\n",
        "\n",
        "# 데이터셋 분할 (train, validation, test)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x_images, y_labels, test_size=0.4, random_state=42)  # 60% train\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n",
        "\n",
        "# Encoder 모델 정의\n",
        "def build_encoder(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    return Model(inputs, encoded, name=\"encoder\")\n",
        "\n",
        "# Classifier 모델 정의\n",
        "def build_classifier(input_shape):\n",
        "    encoder = build_encoder(input_shape)\n",
        "    encoder_output = encoder.output\n",
        "    flat = layers.Flatten()(encoder_output)\n",
        "    dense = layers.Dense(64, activation='relu')(flat)\n",
        "    output = layers.Dense(1, activation='sigmoid')(dense)  # 이진 분류\n",
        "    classifier = Model(encoder.input, output, name=\"classifier\")\n",
        "    return classifier\n",
        "\n",
        "# 분류기 모델 생성\n",
        "input_shape = (128, 128, 3)\n",
        "classifier = build_classifier(input_shape)\n",
        "\n",
        "# 분류 모델 컴파일\n",
        "classifier.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 분류기 모델 학습\n",
        "classifier.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Grad-CAM Heatmap 계산\n",
        "def get_gradcam_heatmap(model, image, class_idx):\n",
        "    # 모델의 마지막 Conv2D 레이어를 찾음\n",
        "    last_conv_layer_name = None\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "            last_conv_layer_name = layer.name\n",
        "            break\n",
        "\n",
        "    # 마지막 Conv2D 레이어의 출력 부분 모델과 이후의 분류기 부분 모델을 생성\n",
        "    last_conv_layer_model = Model(inputs=model.inputs, outputs=model.get_layer(last_conv_layer_name).output)\n",
        "    classifier_model = Model(inputs=model.get_layer(last_conv_layer_name).output, outputs=model.outputs)\n",
        "\n",
        "    # 이미지를 텐서로 변환\n",
        "    image = tf.convert_to_tensor(np.expand_dims(image, axis=0), dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Conv 레이어 출력 및 예측값 계산\n",
        "        conv_output = last_conv_layer_model(image)\n",
        "        tape.watch(conv_output)\n",
        "        preds = classifier_model(conv_output)\n",
        "        class_output = preds[:, class_idx]\n",
        "\n",
        "    # 그라디언트 계산\n",
        "    grads = tape.gradient(class_output, conv_output)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # heatmap 생성\n",
        "    conv_output = conv_output[0]\n",
        "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
        "\n",
        "    # heatmap 정규화\n",
        "    heatmap = np.maximum(heatmap, 0)  # 음수 제거\n",
        "    heatmap /= np.max(heatmap) + 1e-8  # 정규화\n",
        "\n",
        "    # Heatmap이 비어있는지 확인\n",
        "    if np.all(heatmap == 0):\n",
        "        print(\"Warning: Generated heatmap is empty.\")\n",
        "    return heatmap  # numpy 배열로 반환\n",
        "\n",
        "\n",
        "# Heatmap에서 Bounding Box 계산\n",
        "def get_gradcam_bbox(heatmap, threshold=0.5):\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    threshold_value = threshold * 255\n",
        "    coords = np.column_stack(np.where(heatmap >= threshold_value))\n",
        "    y_min, x_min = coords.min(axis=0)\n",
        "    y_max, x_max = coords.max(axis=0)\n",
        "    return x_min, y_min, x_max, y_max\n",
        "\n",
        "# Grad-CAM + Bounding Box 시각화\n",
        "def display_gradcam_with_bbox(model, image, predicted_class, label, threshold=0.5):\n",
        "    # Grad-CAM heatmap 계산\n",
        "    heatmap = get_gradcam_heatmap(model, image, predicted_class)\n",
        "\n",
        "    if heatmap is None or np.all(heatmap == 0):\n",
        "        print(\"No heatmap generated, skipping visualization.\")\n",
        "        return\n",
        "\n",
        "    # Heatmap을 원본 이미지 크기로 조정\n",
        "    heatmap_resized = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    heatmap_resized = np.uint8(255 * heatmap_resized)  # heatmap의 값 0~255로 변환\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)  # 색상 맵 적용\n",
        "\n",
        "    # 원본 이미지와 heatmap 합성\n",
        "    superimposed_img = cv2.addWeighted(heatmap_color, 0.3, np.uint8(image * 255), 0.7, 0)  # 비율 0.3:0.7로 조정\n",
        "\n",
        "    # Threshold를 적용하여 bounding box를 찾기\n",
        "    ret, thresh = cv2.threshold(heatmap_resized, threshold * 255, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # 각 contour에 대해 bounding box를 그리기\n",
        "    for contour in contours:\n",
        "        if cv2.contourArea(contour) > 100:  # 작은 영역은 무시\n",
        "            (x, y, w, h) = cv2.boundingRect(contour)\n",
        "            cv2.rectangle(superimposed_img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "            # 딥페이크/리얼 텍스트 표시\n",
        "            cv2.putText(superimposed_img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "    # 결과 이미지 시각화\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 테스트 이미지와 Grad-CAM 시각화\n",
        "test_image = x_test[0]\n",
        "predicted_class = int(classifier.predict(np.expand_dims(test_image, axis=0))[0, 0] > 0.5)\n",
        "label = \"Deepfake\" if predicted_class == 0 else \"Real\"\n",
        "display_gradcam_with_bbox(classifier, test_image, predicted_class, label, threshold=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BieoqLzUDwGO",
        "outputId": "f57edc05-4e82-4a0a-d710-d83792a4be81"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.8617 - loss: 0.1279 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.7718e-42 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7df155d292d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step\n",
            "Warning: Generated heatmap is empty.\n",
            "No heatmap generated, skipping visualization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_108']. Received: the structure of inputs=*\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EicbTDOa2wx",
        "outputId": "6257dc70-136b-427c-cbc9-ae4933f51cfa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: setup.py: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import Sequence\n",
        "\n",
        "\n",
        "# 이미지 로딩 및 전처리 함수\n",
        "def load_images(image_paths, target_size=(128, 128)):\n",
        "    images = []\n",
        "    for img_path in image_paths:\n",
        "        img = load_img(img_path, target_size=target_size)\n",
        "        img = img_to_array(img) / 255.0  # 정규화\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Google Drive에서 데이터 로드 및 준비\n",
        "deepfake_folder = '/content/drive/MyDrive/deepfake/'  # 실제 경로로 변경\n",
        "image_paths = [os.path.join(deepfake_folder, f) for f in os.listdir(deepfake_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# 이미지 데이터 로드\n",
        "x_images = load_images(image_paths)\n",
        "\n",
        "# 라벨 생성 (FAKE: 0, REAL: 1)\n",
        "y_labels = []\n",
        "for img_path in image_paths:\n",
        "    if 'fake' in img_path.lower():\n",
        "        y_labels.append(0)  # FAKE\n",
        "    else:\n",
        "        y_labels.append(1)  # REAL\n",
        "y_labels = np.array(y_labels)\n",
        "\n",
        "# 데이터셋 분할 (train, validation, test)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x_images, y_labels, test_size=0.4, random_state=42)  # 60% train\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n",
        "\n",
        "# 데이터 증강 (augmentation)\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,        # 이미지 회전\n",
        "    width_shift_range=0.1,    # 가로 이동\n",
        "    height_shift_range=0.1,   # 세로 이동\n",
        "    horizontal_flip=True      # 수평 뒤집기\n",
        ")\n",
        "\n",
        "# 훈련 데이터에 데이터 증강 적용\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Encoder 모델 정의\n",
        "def build_encoder(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    return Model(inputs, encoded, name=\"encoder\")\n",
        "\n",
        "# Classifier 모델 정의 (드롭아웃과 L2 정규화 추가)\n",
        "def build_classifier_with_dropout(input_shape):\n",
        "    encoder = build_encoder(input_shape)\n",
        "    encoder_output = encoder.output\n",
        "    flat = layers.Flatten()(encoder_output)\n",
        "    dense = layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(flat)  # L2 정규화 추가\n",
        "    dropout = layers.Dropout(0.5)(dense)  # 드롭아웃 추가\n",
        "    output = layers.Dense(1, activation='sigmoid')(dropout)  # 이진 분류\n",
        "    classifier = Model(encoder.input, output, name=\"classifier_with_dropout\")\n",
        "    return classifier\n",
        "\n",
        "# 분류기 모델 생성 (드롭아웃 포함)\n",
        "input_shape = (128, 128, 3)\n",
        "classifier = build_classifier_with_dropout(input_shape)\n",
        "\n",
        "# 분류 모델 컴파일\n",
        "classifier.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "epochs = 50  # 에포크 수\n",
        "classifier.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_val, y_val), verbose=1)\n",
        "\n",
        "# Grad-CAM 함수 수정\n",
        "def get_gradcam_heatmap(model, image, class_idx):\n",
        "    # 모델 구조에서 마지막 Conv2D 레이어를 자동으로 탐색\n",
        "    last_conv_layer_name = None\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, layers.Conv2D):\n",
        "            last_conv_layer_name = layer.name\n",
        "            break\n",
        "\n",
        "    if not last_conv_layer_name:\n",
        "        raise ValueError(\"No Conv2D layer found in the model.\")\n",
        "\n",
        "    # 마지막 Conv2D 레이어를 기준으로 모델을 다시 정의\n",
        "    last_conv_layer_model = Model(inputs=model.inputs, outputs=model.get_layer(last_conv_layer_name).output)\n",
        "    classifier_model = Model(inputs=model.get_layer(last_conv_layer_name).output, outputs=model.outputs)\n",
        "\n",
        "    # 이미지 전처리\n",
        "    image_input = np.expand_dims(image, axis=0)  # 배치 차원 추가\n",
        "    image_input = tf.convert_to_tensor(image_input, dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image_input)\n",
        "        conv_output = last_conv_layer_model(image_input)\n",
        "        preds = classifier_model(conv_output)\n",
        "        class_output = preds[:, class_idx]\n",
        "\n",
        "    # 그라디언트 계산\n",
        "    grads = tape.gradient(class_output, conv_output)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))  # 채널 차원에서 평균\n",
        "\n",
        "    # Heatmap 생성\n",
        "    conv_output = conv_output[0]  # (height, width, channels) 형태로 변환\n",
        "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0)  # 음수 제거\n",
        "    heatmap /= np.max(heatmap) + 1e-8  # 정규화\n",
        "\n",
        "    return heatmap  # 생성된 Heatmap 반환\n",
        "\n",
        "# Grad-CAM + Bounding Box 시각화\n",
        "def display_gradcam_with_bbox(model, image, predicted_class, label, threshold=0.5):\n",
        "    # Grad-CAM heatmap 계산\n",
        "    heatmap = get_gradcam_heatmap(model, image, predicted_class)\n",
        "\n",
        "    if heatmap is None or np.all(heatmap == 0):\n",
        "        print(\"No heatmap generated, skipping visualization.\")\n",
        "        return\n",
        "\n",
        "    # Heatmap을 원본 이미지 크기로 조정\n",
        "    heatmap_resized = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    heatmap_resized = np.uint8(255 * heatmap_resized)  # heatmap의 값 0~255로 변환\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)  # 색상 맵 적용\n",
        "\n",
        "    # 원본 이미지와 heatmap 합성\n",
        "    superimposed_img = cv2.addWeighted(heatmap_color, 0.3, np.uint8(image * 255), 0.7, 0)  # 비율 0.3:0.7로 조정\n",
        "\n",
        "    # Threshold를 적용하여 bounding box를 찾기\n",
        "    ret, thresh = cv2.threshold(heatmap_resized, threshold * 255, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # 각 contour에 대해 bounding box를 그리기\n",
        "    for contour in contours:\n",
        "        if cv2.contourArea(contour) > 100:  # 작은 영역은 무시\n",
        "            (x, y, w, h) = cv2.boundingRect(contour)\n",
        "            cv2.rectangle(superimposed_img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "            # 딥페이크/리얼 텍스트 표시\n",
        "            cv2.putText(superimposed_img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "    # 결과 이미지 시각화\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 테스트 이미지와 Grad-CAM 시각화\n",
        "test_image = x_test[0]  # 이미 (128, 128, 3) 형태임\n",
        "predicted_class = int(classifier.predict(np.expand_dims(test_image, axis=0))[0, 0] > 0.5)  # 예측을 위해서는 배치 차원이 필요합니다.\n",
        "label = \"Deepfake\" if predicted_class == 0 else \"Real\"\n",
        "display_gradcam_with_bbox(classifier, test_image, predicted_class, label, threshold=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JV_DX-1gcu9",
        "outputId": "1c0198da-7c84-417b-cc8a-ce5f0372b3b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 212ms/step - accuracy: 0.8477 - loss: 0.2330 - val_accuracy: 1.0000 - val_loss: 0.0688\n",
            "Epoch 2/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.0633 - val_accuracy: 1.0000 - val_loss: 0.0474\n",
            "Epoch 3/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0434 - val_accuracy: 1.0000 - val_loss: 0.0331\n",
            "Epoch 4/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.0305 - val_accuracy: 1.0000 - val_loss: 0.0239\n",
            "Epoch 5/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.0222 - val_accuracy: 1.0000 - val_loss: 0.0177\n",
            "Epoch 6/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 0.0134\n",
            "Epoch 7/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0126 - val_accuracy: 1.0000 - val_loss: 0.0103\n",
            "Epoch 8/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 0.0080\n",
            "Epoch 9/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
            "Epoch 10/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0051\n",
            "Epoch 11/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
            "Epoch 12/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0031\n",
            "Epoch 13/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0040\n",
            "Epoch 14/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0031\n",
            "Epoch 15/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
            "Epoch 16/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 17/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 18/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 1.0000 - val_loss: 8.4309e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 8.1205e-04 - val_accuracy: 1.0000 - val_loss: 7.1946e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 21/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 8.4795e-04 - val_accuracy: 1.0000 - val_loss: 5.2516e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 4.7920e-04 - val_accuracy: 1.0000 - val_loss: 3.8114e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 3.6726e-04 - val_accuracy: 1.0000 - val_loss: 3.1794e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 3.1089e-04 - val_accuracy: 1.0000 - val_loss: 2.5487e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 2.4084e-04 - val_accuracy: 1.0000 - val_loss: 2.0201e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 2.5767e-04 - val_accuracy: 1.0000 - val_loss: 2.1645e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 3.8001e-04 - val_accuracy: 1.0000 - val_loss: 0.0115\n",
            "Epoch 28/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 0.0094\n",
            "Epoch 29/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
            "Epoch 30/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 6.0557e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 4.7094e-04 - val_accuracy: 1.0000 - val_loss: 2.0583e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 3.8044e-04 - val_accuracy: 1.0000 - val_loss: 7.6990e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 5.7426e-04 - val_accuracy: 1.0000 - val_loss: 1.6835e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 1.3291e-04 - val_accuracy: 1.0000 - val_loss: 1.0424e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 1.0012e-04 - val_accuracy: 1.0000 - val_loss: 2.6464e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 1.9389e-04 - val_accuracy: 1.0000 - val_loss: 7.7166e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 6.2646e-05 - val_accuracy: 1.0000 - val_loss: 3.5332e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 4.2779e-05 - val_accuracy: 1.0000 - val_loss: 7.5156e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 6.5581e-05 - val_accuracy: 1.0000 - val_loss: 7.2011e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 5.8037e-05 - val_accuracy: 1.0000 - val_loss: 2.3028e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 2.1271e-05 - val_accuracy: 1.0000 - val_loss: 1.5018e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 1.4973e-05 - val_accuracy: 1.0000 - val_loss: 1.5140e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 1.3919e-05 - val_accuracy: 1.0000 - val_loss: 1.0857e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 2.7622e-05 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
            "Epoch 45/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
            "Epoch 46/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 7.4938e-04 - val_accuracy: 1.0000 - val_loss: 1.1915e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 2.3771e-04 - val_accuracy: 1.0000 - val_loss: 2.1331e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 2.4177e-04 - val_accuracy: 1.0000 - val_loss: 1.1391e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 8.0955e-05 - val_accuracy: 1.0000 - val_loss: 2.3776e-05\n",
            "Epoch 50/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0080\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n",
            "No heatmap generated, skipping visualization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_65']. Received: the structure of inputs=*\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}